\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{mnistlecun}
\citation{Huang:2009:SFD:1704175.1704194}
\citation{Jou:2004:HNR:1123321.1123360}
\citation{726791}
\citation{mnistTheano}
\citation{mnistlecun}
\citation{comp598p3}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{\thepage }{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{\thepage }{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{\thepage }{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data Preprocessing Methods}{\thepage }{subsection.3.1}}
\citation{Bishop:2006:PRM:1162264}
\citation{pineaul5}
\citation{726791}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Feature Design and Selection}{\thepage }{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Algorithm Selection}{\thepage }{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Baseline: Gaussian Naive Bayes}{\thepage }{subsubsection.3.3.1}}
\newlabel{eq:max}{{1}{\thepage }{Baseline: Gaussian Naive Bayes\relax }{equation.3.1}{}}
\newlabel{eq:pdf}{{2}{\thepage }{Baseline: Gaussian Naive Bayes\relax }{equation.3.2}{}}
\newlabel{eq:mean}{{3}{\thepage }{Baseline: Gaussian Naive Bayes\relax }{equation.3.3}{}}
\newlabel{eq:var}{{4}{\thepage }{Baseline: Gaussian Naive Bayes\relax }{equation.3.4}{}}
\newlabel{eq: argmax}{{5}{\thepage }{Baseline: Gaussian Naive Bayes\relax }{equation.3.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Neural Net}{\thepage }{subsubsection.3.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Linear SVM }{\thepage }{subsubsection.3.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Convolutional Neural Networks}{\thepage }{subsubsection.3.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Optimization}{\thepage }{subsection.3.4}}
\newlabel{eq:log}{{6}{\thepage }{Optimization\relax }{equation.3.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Convolutional Neural Networks}{\thepage }{subsubsection.3.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Testing and Validation}{\thepage }{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Parameter Selection}{\thepage }{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Baseline: Naive Bayes}{\thepage }{subsubsection.4.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Neural Net}{\thepage }{subsubsection.4.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Accuracy versus different Naive Bayes using the original data, $L1$-normalized data, and $L2$-normalized data (train set size = 40,000 and test set size = 10,000)\relax }}{\thepage }{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gnbparam}{{1}{\thepage }{Accuracy versus different Naive Bayes using the original data, $L1$-normalized data, and $L2$-normalized data (train set size = 40,000 and test set size = 10,000)\relax \relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Validation error rate of different ConvNet model sizes during training\relax }}{\thepage }{figure.caption.3}}
\newlabel{fig:conv_train}{{2}{\thepage }{Validation error rate of different ConvNet model sizes during training\relax \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Linear SVM }{\thepage }{subsubsection.4.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Convolutional Neural Networks}{\thepage }{subsubsection.4.1.4}}
\bibstyle{abbrv}
\bibdata{sigproc}
\bibcite{Bishop:2006:PRM:1162264}{1}
\bibcite{Huang:2009:SFD:1704175.1704194}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Normalized confusion matrix for Gaussian Naive Bayes with train set size = 40,000 and test set size = 10,000\relax }}{\thepage }{figure.caption.4}}
\newlabel{fig:gnb_cm}{{3}{\thepage }{Normalized confusion matrix for Gaussian Naive Bayes with train set size = 40,000 and test set size = 10,000\relax \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Testing Results Analysis}{\thepage }{subsection.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Baseline: Naive Bayes}{\thepage }{subsubsection.4.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Neural Net}{\thepage }{subsubsection.4.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Linear SVM }{\thepage }{subsubsection.4.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Convolutional Neural Networks}{\thepage }{subsubsection.4.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{\thepage }{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Normalized confusion matrix for Convolutional Neural Nets with train set size = 40,000 and test set size = 10,000\relax }}{\thepage }{figure.caption.5}}
\newlabel{fig:conv_conf}{{4}{\thepage }{Normalized confusion matrix for Convolutional Neural Nets with train set size = 40,000 and test set size = 10,000\relax \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}References}{\thepage }{section.6}}
\bibcite{Jou:2004:HNR:1123321.1123360}{3}
\bibcite{726791}{4}
\bibcite{mnistlecun}{5}
\bibcite{pineaul5}{6}
\bibcite{comp598p3}{7}
\bibcite{mnistTheano}{8}
